<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html> <!--<![endif]-->
<!-- testing commit -->
<head>
<meta charset="utf-8">
<title>5th International Workshop on Big Surveillance Data Analysis and Processing (Big-SURV) @ ICME 2023</title>
<meta name="description" content="5th International Workshop on Big Surveillance Data Analysis and Processing (Big-SURV)">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">
<meta name="keywords" content="Big Data, Surveillance, Data Analysis, Data Processing, Artificial Intelligence, AI, Computer Vision, ICME 2022, ICME Workshops, ICME">
<link rel="stylesheet" href="css/bootstrap.min.css">

<!-- Main CSS -->
<link rel="stylesheet" href="css/main.css">
<!-- Fonts -->
<link rel="stylesheet" href="css/icon/font-awesome.css">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700,900" rel="stylesheet">
<!-- Linea Basic -->
<link rel="stylesheet" href="css/icon/lineabasic.css">
<!-- Animate.css -->
<link rel="stylesheet" href="css/animate.min.css">
<!-- Magnific Popup -->
<link rel="stylesheet" href="css/magnific-popup.css">
<!-- style -->
<link rel="stylesheet" href="css/style.css">

  <!-- <link rel="icon" href="img/favicon.ico"> -->

<script src="js/vendor/modernizr.js"></script>


</head>
<body id="home" data-spy="scroll" data-offset="50" data-target=".navbar-default">

<!-- Preloader -->
<div class="preloader">
  <div class="preloader10">
    <span></span>
    <span></span>
  </div>
</div>


<!-- Navbar -->
<nav class="navbar navbar-default navbar-fixed-top">
  <div class="container">
    <div class="row">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <!--<a target="_blank" href="https://2022.ieeeicme.org/">-->
          <img src="img/empty_logo.png" alt="FOLIO LOGO" style=" height:60px;" />
        </a>
        <b style="font-size:1.2em;"><a href="http://2023.ieeeicme.org/" target="_blank">5th BIG-Surv Workshop @ ICME 2023</a></b>
      </div>
      <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1" >
        <ul class="nav navbar-nav navbar-right" style="padding-top:6px;">
          <li><a href="#home">Home</a></li>
          <!-- <li><a href="#portfolio">About</a></li> -->
          <!--<li><a href="#winners">Winners</a></li>-->
          <!--<li><a href="#poster">Posters</a></li>-->
          <li><a href="#topics">Scope</a></li>
          <!-- <li><a href="#acceptedpapers">Accepted Papers</a></li> -->
          <!-- <li><a href="#winners">Challenge Winners</a></li> -->
          <li><a href="#cfp"><strong>CFP</strong></a></li>
          <li><a href="#schedule">Schedule</a></li>
          <!-- <li><a href="#challenge">Challenges</a></li> -->
          <li><a href="#speaker">Speakers</a></li>
          <li><a href="#team">Organizers</a></li>
          <!--<li><a href="#association">Contact</a></li>-->
          <li class="dropdown"><a class="dropdown-toggle" data-toggle="dropdown" href="#history">History <span class="caret"></span></a>
            <ul class="dropdown-menu">
              <li><a href="index_2021.html" target="_blank">BIG-Surv-2021</a></li>
              <li><a href="index_2022.html" target="_blank">BIG-Surv-2022</a></li>
            </ul>
          </li>
        </ul>
      </div>
    </div>
  </div>
</nav>



<!-- Header -->
<header id="revolution-home" class="revolution-home">
  <div id="rev_slider_490_1_wrapper" class="rev_slider_wrapper fullwidthbanner-container" data-alias="image-hero39" data-source="gallery" style="margin:0px auto;background-color:transparent;padding:0px;margin-top:0px;margin-bottom:0px;">
    <!-- START REVOLUTION SLIDER 5.3.0.2 fullwidth mode -->
      <div id="rev_slider_490_1" class="rev_slider fullwidthabanner" style="display:none;" data-version="5.3.0.2">
    <ul>    <!-- SLIDE  -->
      <li data-index="rs-1699" data-transition="zoomout" data-slotamount="default" data-hideafterloop="0" data-hideslideonmobile="off"  data-easein="Power4.easeInOut" data-easeout="Power4.easeInOut" data-masterspeed="2000"  data-thumb="img/sliders/SaltLake-100x50.jpg"  data-rotate="0"  data-saveperformance="off"  data-title="Intro" data-param1="" data-param2="" data-param3="" data-param4="" data-param5="" data-param6="" data-param7="" data-param8="" data-param9="" data-param10="" data-description="">
        <!-- MAIN IMAGE -->
        <img style="max-width:100% ! important; height:auto!important;" src="img/ICME2023_banner.jpg"  alt=""  data-bgposition="center center" data-bgfit="cover" data-bgrepeat="no-repeat" data-bgparallax="10" class="rev-slidebg" data-no-retina>
        <!-- LAYERS -->

        <!-- LAYER NR. 1 -->
        <div class="tp-caption tp-shape  "
           id="slide-1699-layer-10"
           data-x="['center','center','center','center']" data-hoffset="['0','0','0','0']"
           data-y="['middle','middle','middle','middle']" data-voffset="['0','0','0','0']"
                data-width="full"
          data-height="full"
          data-whitespace="nowrap"

          data-type="shape"
          data-basealign="slide"
          data-responsive_offset="on"
          data-responsive="off"
          data-frames='[{"from":"opacity:0;","speed":1500,"to":"o:1;","delay":750,"ease":"Power3.easeInOut"},{"delay":"wait","speed":300,"ease":"nothing"}]'
          data-textAlign="['left','left','left','left']"
          data-paddingtop="[0,0,0,0]"
          data-paddingright="[0,0,0,0]"
          data-paddingbottom="[0,0,0,0]"
          data-paddingleft="[0,0,0,0]"

          style="z-index: 5;text-transform:left;background-color:rgba(0, 0, 0, 0.40);border-color:rgba(0, 0, 0, 0.50);border-width:0px; max-width:100% ! important;"> </div>

        <!-- LAYER NR. 3 -->
        <div class="tp-caption NotGeneric-Title   tp-resizeme"
           id="slide-1699-layer-1"
           data-x="['center','center','center','center']" data-hoffset="['0','0','0','0']"
           data-y="['middle','middle','middle','middle']" data-voffset="['0','0','-22','-29']"
                data-fontsize="['70','70','70','50']"
          data-lineheight="['70','70','70','50']"
          data-width="none"
          data-height="none"
          data-whitespace="nowrap"

          data-type="text"
          data-responsive_offset="on"

          data-frames='[{"from":"z:0;rX:0deg;rY:0;rZ:0;sX:1.5;sY:1.5;skX:0;skY:0;opacity:0;","mask":"x:0px;y:0px;","speed":1500,"to":"o:1;","delay":1000,"ease":"Power3.easeInOut"},{"delay":"wait","speed":1000,"to":"y:[100%];","mask":"x:inherit;y:inherit;","ease":"Power2.easeInOut"}]'
          data-textAlign="['center','center','center','center']"
          data-paddingtop="[10,10,10,10]"
          data-paddingright="[10,10,10,10]"
          data-paddingbottom="[10,10,10,10]"
             data-paddingleft="[10,10,10,10]" style="z-index: 7; text-transform:left;"> 5th International Workshop on</br>Big Surveillance Data Analysis and Processing </br> (BIG-Surv) </br>
          <div style="font-size: 30px;">@ ICME 2023 (Brisbane, Australia)</div>
          <!--<div style="font-size: 1vw ! important;">June 14th - 19th, 2020 Seattle, WA</div>-->
          <!--and 2D/3D Synthesis Synthesis, <br>-->
          <!--and the third Look Into Person (LIP) Challenge-->
        </div>
<!-- LAYER NR. 4 -->
<!--        <div class="tp-caption NotGeneric-Title   tp-resizeme"
           id="slide-1699-layer-1"
           data-x="['center','center','center','center']" data-hoffset="['0','0','0','0']"
           //data-y="['middle','middle','middle','middle']"
             data-voffset="['0','0','-22','-29']"
                data-fontsize="['70','70','70','50']"
          data-lineheight="['42','42','42','30']"
          data-width="none"
          data-height="none"
          data-whitespace="nowrap"

          data-type="text"
          data-responsive_offset="on"

          data-frames='[{"from":"z:0;rX:0deg;rY:0;rZ:0;sX:1.5;sY:1.5;skX:0;skY:0;opacity:0;","mask":"x:0px;y:100px;","speed":1500,"to":"o:1;","delay":1000,"ease":"Power3.easeInOut"},{"delay":"wait","speed":1000,"to":"y:[100%];","mask":"x:inherit;y:inherit;","ease":"Power2.easeInOut"}]'
          data-textAlign="['center','center','center','center']"
          data-paddingtop="[10,10,10,10]"
          data-paddingright="[0,0,0,0]"
          data-paddingbottom="[10,10,10,10]"
          data-paddingleft="[0,0,0,0]"

          style="z-index: 7; white-space:pre;text-transform:left;"><a href="http://cvpr2017.thecvf.com/">--in conjunction with CVPR 2017,Honolulu,Hawaii,USA,July 21,2017</a></div>
 -->

        <!-- LAYER NR. 5 -->
        <div class="tp-caption NotGeneric-CallToAction rev-btn "
           id="slide-1699-layer-7"
           data-x="['center','center','center','center']" data-hoffset="['0','0','0','0']"
           data-y="['middle','middle','middle','middle']" data-voffset="['180','180','80','65']"
                data-width="none"
          data-height="none"
          data-whitespace="nowrap"
          onmouseover="this.style.cursor='pointer'" onclick="document.location='#portfolio';"
          href="page.html"
          data-type="button"
          data-actions='[{"event":"click","action":"#portfolio",  "offset":"0px","delay":""}]'
          data-responsive_offset="on"
          data-responsive="off"
          data-frames='[{"from":"y:50px;opacity:0;","speed":1500,"to":"o:1;","delay":1250,"ease":"Power4.easeInOut"},{"delay":"wait","speed":1000,"to":"y:[175%];","mask":"x:inherit;y:inherit;s:inherit;e:inherit;","ease":"Power2.easeInOut"},{"frame":"hover","speed":"300","ease":"Power1.easeInOut","to":"o:1;rX:0;rY:0;rZ:0;z:0;","style":"c:rgba(255, 255, 255, 1.00);bc:rgba(255, 255, 255, 1.00);bw:1px 1px 1px 1px;"}]'
          data-textAlign="['left','left','left','left']"
          data-paddingtop="[10,10,10,10]"
          data-paddingright="[30,30,30,30]"
          data-paddingbottom="[10,10,10,10]"
          data-paddingleft="[30,30,30,30]"

          style="z-index: 9; white-space: nowrap;text-transform:left;outline:none;box-shadow:none;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;cursor:pointer;">ABOUT US </div>
      </li>
    </ul>
    <div class="tp-bannertimer tp-bottom" style="visibility: hidden !important;"></div> </div>
    </div><!-- END REVOLUTION SLIDER -->
</header>

<!-- Portfolio -->
<section id="portfolio" class="portfolio">
  <div class="container">
    <div class="row">
      <div class="col-md-12">
        <header class="section-header">
          <h3 class="section-title" data-text="">Introduction</h3>
         <!-- <div class="panel panel-default">
            <div class="panel-body">-->
              <!--
              <p style="text-align:justify; white-space: pre-wrap;font-size: 1.2em;">
              With the rapid growth of video surveillance applications and services, the amount of surveillance videos has become extremely "big" which makes human monitoring tedious and difficult. Therefore, there exists a huge demand for smart surveillance techniques which can perform monitoring in an automatic or semi-automatic way. A number of challenges have arisen in the area of big surveillance data analysis and processing. Firstly, with the huge amount of surveillance videos in storage, video analysis tasks such as event detection, action recognition, and video summarization are of increasing importance in applications including events-of-interest retrieval and abnormality detection. Secondly, semantic data (e.g. objects' trajectory and bounding boxes) has become an essential data type in surveillance systems owing much to the growth of its size and complexity, hence introducing new challenging topics, such as efficient semantic data processing and compression, to the community. Thirdly, with the rapid growth from the static centric-based processing to the dynamic computing among distributed video processing nodes/cameras, new challenges such as multi-camera analysis, person re-identification, or distributed video processing are being issued in front of us. To meet these challenges, there is great need to extend existing approaches or explore new feasible techniques.
              </p>
              -->
              <p style="text-align:justify; white-space: pre-wrap;font-size: 1.2em;">
              With the rapid growth of video surveillance applications and services, the amount of surveillance videos has become extremely "big" which makes human monitoring tedious and difficult. At the same time, new issues concerning privacy and security have also arised. Therefore, there exists a huge demand for smart and secure surveillance techniques which can perform monitoring in an automatic way. Firstly, the huge abundance of video surveillance data in storage gives rise to the importance of video analysis tasks such as event detection, action recognition, video summarization including person re-identification and anomaly detection. Secondly, with the rich abundance of semantics and the multimodality of data extracted from surveillance videos, it is now essential for the community to tackle new challenges, such as efficient multimodal data processing and compression. Thirdly, with the rapid shift from static singular processing to dynamic collaborative computing, it is now vital to consider distributed and multi-camera video processing on edge- and cloud-based cameras, and at the same time, offering privacy-preserving considerations to safeguard the data. This workshop aims challenge the multimedia community towards extending existing approaches or exploring brave and new ideas.
              </p>
              <p style="text-align:justify; white-space: pre-wrap;font-size: 1.2em;">This is the 5th edition of our workshop. The first three were organized in conjunction with <a href="https://signalprocessingsociety.org/blog/icme-2019-2019-ieee-international-conference-multimedia-and-expo">ICME 2019</a> (Shanghai, China), <a href="http://2020.ieeeicme.org/www.2020.ieeeicme.org/index.html" target="_blank">ICME 2020</a> (London, UK), <a href="http://2021.ieeeicme.org/2021.ieeeicme.org/index.html" target="_blank">ICME 2021</a> (Shenzhen, China) and <a href="http://2022.ieeeicme.org/2022.ieeeicme.org/index.html" target="_blank">ICME 2022</a> (Taipei, Taiwan ROC)</p>
              <!--<p style="text-align:justify; white-space: pre-wrap;font-size: 1.2em;">&nbsp;&nbsp;&nbsp;&nbsp;To stimulate the progress on this research topic and attract more researchers to work on this topic, we also organize the forth large-scale Look Into Person (LIP) challenge which includes four competition tasks: multi-person human parsing, multi-person video parsing, image-baesd multi-pose virtual try-on, and video virtual try-on. This fourth LIP challenge mainly extends the third LIP challenge in CVPR 2017, CVPR 2018, and CVPR 2019 by additionally covering a video virtual try-on challenge. For the multi-person human parsing competition task, we will provide 38280 images of crowded scenes with 19 semantic human part labels. For video-based human parsing, 404 video shots with 1-2 minutes will be densely annotated with 19 semantic human part labels. The image-based multi-pose virtual try-on benchmark targets at fitting new in-shop clothes into a person image and show different clothes viewpoints of the person. The benchmark contains 35,687/13,524 person/clothes images and we will split them into 52,236/10,544 person-clothes-pose three-tuples for training/testing, respectively. Our new video virtual try-on challenge aims to transfer the in-shop clothes to a person and generate a virtual try-on video according to a pose sequence. We reasele a new video virtual try-on benchmark which contains 661/130 videos and 160492/31191 frames for training/testing, respectively. In terms of the quality of the image-based multi-pose virtual try-on and video virtual try-on, the quantitative performance will be given via a human subjective perceptual study on AMT platform. The images collected from the real-world scenarios contain humans appearing with challenging poses and views, heavily occlusions, various appearances and low-resolutions. Details about the datasets are available at this link <a href="http://47.100.21.47:9999/index.php" style="color: blue;">LIP</a>. The challenge is conjunction with <a href="http://cvpr2020.thecvf.com/" style="color: blue;">CVPR 2020</a>, Seattle, WA. Challenge participants with the most successful and innovative entries will be invited to present on this workshop. -->
              <!--</p>-->
              <!--<p style="text-align:justify; white-space: pre-wrap;font-size: 1.2em;">Regarding the viability of this workshop, the topic of this workshop is attractive and active. It is very possible that many active researchers would like to attend this workshop (actually the expected number of attendees is 100 from a conservative estimation based on the past publication record on related topics). It is related to yet still clearly different from past workshops as explained below. In addition, we have got confirmation from many renowned professors and researchers in this area and they are either glad to give a keynote speech (as listed in the program) or kindly offer help. We believe this workshop will be a very successful one and it will indeed benefit the progress of this research area significantly.-->
              <!--</p>-->
        </header>
      </div>
    </div>
  </div>
</section>
<!-- &lt;!&ndash; About &ndash;&gt; -->
<!--
&lt;!&ndash; About &ndash;&gt;
<section id="poster" class="about">
  <div class="container">
    <div class="row">
      <div class="col-md-8 col-md-offset-2">
        <header class="section-header">
          <h3 class="section-title font-lg" data-text="">Posters</h3>
          <ul class="lsit-group" style="text-align: left;font-size: 1.2em; padding-left:0 ">
            <li class="list-group-item">
              <b>Location: Note that the workshop posters will be in the Pacific Arena Ballroom (main convention center).<br></b>
            </li>
          <ul class="lsit-group" style="text-align: left;font-size: 1.2em; padding-left:0 ">
            <li class="list-group-item">
              <b>10:00 - 11:00 AM:<br></b>
              Poster Numbers: #34 – 38 <br>
              #34: paper 3,  #35: paper 7, #36 paper 9, <br>
              #37:paper 11, #38: paper 14<br>
            </li>
            <li class="list-group-item">
              <b>3:15 - 4:00 PM<br></b>
              Poster Numbers: #34 – 38 <br>
              #34: paper 16,  #35: paper 21, #36 paper 22, <br>
              #37:paper 26    <br>
            </li>
          </ul>
        </header>

      </div>
    </div>
  </div>
</section>
 -->
 <!-- <section id="virtual address" class="association">
  <div class="container">
    <div class="row">
      <div class="col-md-12">
        <header class="section-header">
          <p style="text-align: left;font-size: 1.2em;white-space: pre-wrap; margin-top: -50px">The address of our virtual meeting is <b><a href="http://cvpr20.com/human-centric-image-video-synthesis/" target="_blank" style="white-space: pre-wrap;">human-centric-image-video-synthesis.</a></b>
          <br> The zoom link is <b><a href="https://zoom.us/j/95207927547?pwd=VGdkSWdVVTBmQk9pZmw4djhNTDFSdz09" target="_blank" style="white-space: pre-wrap;">zoom entrance.</a></b>(Please <b>DO NOT</b> tweet it in other public place.)</p>
        </header>
      </div>
    </div>
  </div>
</section> -->
<HR style="FILTER: alpha(opacity=100,finishopacity=0,style=3)" width="90%" color=#987cb9 SIZE=3>
<!-- About -->
<section id="topics" class="topics">
  <div class="container">
    <div class="row">
      <div class="col-md-8 col-md-offset-2">
        <header class="section-header">
          <!-- <h3 class="section-title font-lg" data-text="">Topics of interest</h3> -->
          <h3 class="section-title" data-text="">Scope &amp; Topics</h3>

          <p style="text-align:left; font-size: 1.2em;">This workshop is intended to provide a forum for researchers and engineers to present their latest innovations and share their experiences on all aspects of design and implementation of new surveillance video analysis and processing techniques. Topics of interests include, but are not limited to:</p>
          <ul class="lsit-group" style="text-align: left;font-size: 1.2em; padding-left:0 ">
            <li class="list-group-item">
              Action/activity recognition, and event detection in surveillance videos
            </li>
            <li class="list-group-item">
              Object detection and tracking in surveillance videos
            </li>
            <li class="list-group-item">
              Multi-camera surveillance networks and applications
            </li>
            <li class="list-group-item">
              Surveillance scene parsing, segmentation, and analysis
            </li>
            <li class="list-group-item">
              Crowd parsing, estimation and analysis
            </li>
            <li class="list-group-item">
              Person, group or object or re-identification
            </li>
            <li class="list-group-item">
              Summarization and synopsis of surveillance videos
            </li>
            <li class="list-group-item">
              Big Data processing in large-scale surveillance systems
            </li>
            <li class="list-group-item">
              Distributed, edge and fog computing for surveillance systems
            </li>
            <li class="list-group-item">
              Data compression in surveillance systems
            </li>
            <li class="list-group-item">
              Low-resolution video analysis and processing: Recognition and object detection, restoration, denoising, enhancement, super-resolution
            </li>
            <li class="list-group-item">
              Surveillance from multiple modalities, not limited to: UAVs, satellite imagery, dash cams, wearables.
            </li>

          </ul>
        </header>

      </div>
    </div>
  </div>
</section>
<!-- Call To Action -->
<!-- <section class="call-to">
  <div class="container">
    <div class="row">
      <div class="col-md-8 col-md-offset-2">
        <article class="action">
          <h5>Intrested about us?</h5>
          <a href="http://sysu-hcp.net/lip" class="btn btn-border" role="button">Learn More</a>
        </article>
      </div>
    </div>
  </div>
</section> -->
<!-- ********************************************************************************************************** -->
<!--
<HR style="FILTER: alpha(opacity=100,finishopacity=0,style=3)" width="90%" color=#987cb9 SIZE=3>
<section id="acceptedpapers" class="about">
  <div class="container">
    <div class="row">
      <div class="col-md-8 col-md-offset-2">
        <header class="section-header">
          <h3 class="section-title" data-text="">Accepted Papers</h3>

          <ul class="lsit-group" style="text-align: left;font-size: 1.2em; padding-left:0 ">
            <li class="list-group-item">
              <b>Epipolar Transformer for Multi-view Human Pose Estimation.</b>
              Yihui He (Carnegie Mellon University)*; Rui Yan (Carnegie Mellon University); Katerina Fragkiadaki (Carnegie Mellon University); Shoou-I Yu (Oculus Research Pittsburgh) <b>(Oral, Best Paper)</b>
            </li>
            <li class="list-group-item">
              <b>Yoga-82: A New Dataset for Fine-grained  Classification of Human Poses.</b>
              Manisha Verma (Osaka University)*; Sudhakar Kumawat (Indian Institute of Technology Gandhinagar); Yuta Nakashima (Osaka University); Shanmuganathan (Indian Institute of Technology (IIT) Gandhinagar) <b>(Oral)</b>
            </li>
            <li class="list-group-item">
              <b>The MTA Dataset for Multi Target Multi Camera Pedestrian Tracking by Weighted Distance Aggregation.</b>
              Philipp Köhl (Fraunhofer IOSB); Andreas Specker (Fraunhofer IOSB); Arne Schumann (Fraunhofer IOSB)* <b>(Oral)</b>
            </li>
            <li class="list-group-item">
              <b>LightTrack: A Generic Framework for Online Top-Down Human Pose Tracking.</b>
              Guanghan Ning (JD Finance America Corporation)*; Heng Huang (University of Pittsburgh & JD Digits) <b>(Poster)</b>
            </li>
            <li class="list-group-item">
              <b>Fine grained pointing recognition for natural drone guidance.</b>
              Oscar Leon  Barbed Perez (Universidad de Zaragoza)*; Pablo Azagra Millan (University of Zaragoza); Lucas Teixeira (ETH Zurich); Margarita Chli (ETH Zurich); Javier Civera (Universidad de Zaragoza); Ana Murillo (Universidad de Zaragoza) <b>(Poster)</b>
            </li>
            <li class="list-group-item">
              <b>Reposing Humans by Warping 3D Features.</b>
              Markus Knoche (RWTH Aachen)*; István Sárándi (RWTH Aachen University); Bastian Leibe (RWTH Aachen University) <b>(Poster)</b>
            </li>
          </ul>
        </header>
      </div>
    </div>
  </div>
</section>
<HR style="FILTER: alpha(opacity=100,finishopacity=0,style=3)" width="90%" color=#987cb9 SIZE=3>
<section id="winners" class="about">
  <div class="container">
    <div class="row">
      <div class="col-md-8 col-md-offset-2">
        <header class="section-header">
          <h3 class="section-title" data-text="">Challenge Winners</h3>
          <ul class="lsit-group" style="text-align: left;font-size: 1.2em; padding-left:0 ">
            <li class="list-group-item">
              <b>Track 1: Multi-Person Human Parsing Challenge Winners:<br></b>
              1st:<br>
              Lu Yang1, Qing Song1, Zhihui Wang1, Songcen Xu2 <br>
              1BUPT PRIV Lab, 2Noah's Ark Lab<br>
              <br>
              2nd:<br>
              Tianfei Zhou1, Wenguan Wang2, Ying Fu3<br>
              1Inception Institute of Artificial Intelligence, 2ETH Zurich, 3Beijing Institute of Technology<br>
              <br>
              3rd:<br>
              Runxin Mao1, Taiwu Sun1, Zhanwang Zhang1, Xiao Tian1<br>
              1Ping An Technology（Shenzhen）Co., Ltd<br>
            </li>
            <li class="list-group-item">
              <b>Track 2: Video Multi-Person Human Parsing Challenge Winners:<br></b>
              1st:<br>
              Lu Yang1, Qing Song1, Zhihui Wang1, Songcen Xu2<br>
              1BUPT PRIV Lab, 2Noah's Ark Lab<br>
              <br>
              2nd:<br>
              Zhanwang Zhang1, Xiao Tian1, Runxin Mao1, Taiwu Sun1<br>
              1Ping An Technology（Shenzhen）Co., Ltd<br>
              <!-- <br>
              3rd:<br>
              Hong Hu, Feng Zhang, Hanbin Dai, Huan Luo, LiangBo Zhou, Mao Ye <br>
              University of Electronic Science and Technology of China(UESTC)<br> -->
            <!--
            </li>
            <li class="list-group-item">
              <b>Track 3: Image-based Multi-pose Virtual Try-on Challenge Winners: <br></b>
              1st: <br>
              Chieh-Yun Chen1, Hong-Han Shuai1, Wen-Huang Cheng1 <br>
              1National Chiao Tung University<br>
               <br>
              2nd: <br>
              Thai Thanh Tuan1, Matiur Rahman Minar1, Heejune Ahn1<br>
              1Seoul National University of Science and Technology<br>
              <br>
              3rd: <br>
              Zhipeng Luo1, Junfeng Zheng1, Zhenyu Xu1, FengNi1<br>
              1DeepBlue Technology(Shanghai) Co., Ltd<br>
            </li>
            <li class="list-group-item">
              <b>Track 4: Video Virtual Try-on Challenge Winners:<br></b>
              1st:<br>
              Andrew Jong1, Gaurav Kuppa1, Xin Liu2, Teng-Sheng Moh1, Ziwei Liu2<br>
              1San José State University, 2The Chinese University of Hong Kong<br>
              <br>
              2nd:<br>
              Haien Zeng1<br>
              1Sun Yat-Sen University<br>
            </li>
            <li class="list-group-item">
              <b>Track 5: Dark Complexion Portrait Segmentation Challenge Winners:<br></b>
              1st:<br>
              Chenhang Zhou1, Guoqiang Shang1, Ben Ying1, Leheng Zhang1, Jianliang Lan1, Longan Xiao1, Jiangtao Li1<br>
              1Shanghai Transsion Information Technology Limited<br>
              <br>
              2nd:<br>
              Bingke Zhu, Peigeng Ding, Xiaomei Zhang, Yingying Chen, Ming Tang, Jinqiao Wang<br>
              1National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, 2School of Artificial Intelligence, University of Chinese Academy of Sciences<br>
              <br>
              3rd: <br>
              Minh-Quan Le1,2, Hoang-Phuc Nguyen-Dinh1,2, Anh-Minh Nguyen1,2 , Tam V. Nguyen3, Minh-Triet Tran1,2<br>
              1University of Science, VNU-HCM, Vietnam, 2Vietnam National University, Ho Chi Minh City, Vietnam, 3University of Dayton, U.S.A<br>
            </li>
          </ul>
        </header>
      </div>
    </div>
  </div>
</section>

<HR style="FILTER: alpha(opacity=100,finishopacity=0,style=3)" width="90%" color=#987cb9 SIZE=3> -->
<!-- call for papers -->
<section id="cfp" class="cfp">
  <div class="container">
    <div class="row">
      <div class="col-md-12">
        <header class="section-header">
          <h3 class="section-title" data-text="">Call for Papers</h3>
          <p align="center">Download the Call for Papers here<br />
            <a href="/docs/CFP-5th-BIGSurv-Workshop-ICME2023.pdf" target="_blank"><img src="/img/pdf-icon.png" height="75" width="75" /></a>
          </p>
          <table class="table">
            <!-- <caption style="font-weight: bolder;text-align: center;color: black;font-size:1.3em"><br>Paper Submission<br><br></caption>
           -->
            <tbody style="font-size: 1.2em">
              <tr>
                <td style="font-weight: bolder;color: black;font-size:1.1em;vertical-align: middle;">News</td>
              </tr>
              <!--
              <tr>
                 <td><ul>The conference zoom link is: <a href="https://us02web.zoom.us/j/89557731422?pwd=Mmp5UlRycXlkYTliRXppcG1PRWJGUT09"><font color="orange">https://us02web.zoom.us/j/89557731422?pwd=Mmp5UlRycXlkYTliRXppcG1PRWJGUT09</font></a>
              </tr>
              <tr>
                <td><ul>The conference schedule can be found <a href="#schedule"><font color="orange">here</font></a>
              </tr>
              <tr>
                <td><ul>List of paper IDs of accepted papers can be found <a href="./accepted_ID.txt"><font color="orange">here</font></a>
              </tr>
            -->
              <tr>
                <td style="font-weight: bolder;color: black;font-size:1.1em;vertical-align: middle;">Important Dates</td>
              </tr>
              <tr>
                <td><ul>Paper Submission Due Date: <strike>March 23, 2023</strike> <font color="red">April 6, 2023</font>
                </ul></td>
              </tr>
              <tr>
                <td><ul>Notification of Acceptance/Rejection: April 23, 2023</font>
                </ul></td>
              </tr>
              <tr>
                <td><ul>Camera-Ready Due Date (<b>firm deadline</b>): <font color="red">May 1, 2023</font>
                </ul></td>
              </tr>
              <tr>
                <td><ul>Workshop Date and Venue: July 10, 2023 (TBC)
                </ul></td>
              </tr>
              <!-- next section -->
              <tr>
                <td style="font-weight: bolder;color: black;font-size:1.1em;vertical-align: middle;">Format Requirements &amp; Templates</td>
              </tr>
              <tr>
                <td><ul>Length: <b> Papers must be no longer than 6 pages, including all text, figures, and references.</ul></td>
              </tr>
              <tr>
                <td><ul>Format: <b> Workshop papers have the same format as regular papers. See the templates below. Submitted paper does not need to be double blind.</ul></td>
              </tr>
              <!--
              <tr>
                <td><ul>Download: <a target="_blank" style="color:blue;" href="http://2023.ieeeicme.org/assets/templates/icme2022template.docx">Word Template &amp; Sample (zip)</a></ul></td>
              </tr>
              <tr>
                <td><ul>Download: <a target="_blank" style="color:blue;" href="http://2023.ieeeicme.org/assets/templates/icme2022template.zip">LaTeX Template &amp; Sample (zip)</a></ul></td>
              </tr>

              <tr>
                <td><ul>Important: A complete paper should be submitted using the above templates.</ul></td>
              </tr>
            -->
              <!-- next section -->
              <tr>
                <td style="font-weight: bolder;color: black;font-size:1.1em;vertical-align: middle;">Submission Details</td>
              </tr>
              <tr>
                <td><ul>Paper Submission Site: <a target="_blank" style="color:blue;" href="https://cmt3.research.microsoft.com/ICMEW2023">https://cmt3.research.microsoft.com/ICMEW2023</a></br>(Please make sure your paper is submitted to the correct track)</br>Submissions may be accompanied by up to 20 MB of supplemental material following the same guidelines as regular and special session papers.</ul></td>
              </tr>
              <tr>
                <td><ul>Review: <b> Reviews will be handled directly by the Organizers and the Technical Program Committee (TPC).</ul></td>
              </tr>
              <tr>
                <td><ul>Presentation guarantee: <b> As with accepted Regular and Special Session papers, accepted Workshop papers must be registered by the author deadline and presented at the conference; otherwise they will not be included in IEEE Xplore. A workshop paper is covered by a full-conference registration only.</ul></td>
              </tr>

            </tbody>
          </table>
          <!--
          <table class="table">
            <caption style="font-weight: bolder;text-align: center;color: black;font-size:1.3em"><br>Best Paper Award<br><br></caption>

            <tbody style="font-size: 1.2em">
              <tr>
                <td><ul>We set the Best Paper Award for one outstanding paper from all of the accepted papers for its novelty and significant contribution. </ul> </td>
              </tr>
            </tbody>
          </table>
          -->
        </header>
      </div>
    </div>
  </div>
</section>

<!--
<HR style="FILTER: alpha(opacity=100,finishopacity=0,style=3)" width="90%" color=#987cb9 SIZE=3>
<section id="challenge" class="portfolio">
    <div class="container">
    <div class="row">
      <div class="col-md-12">
        <header class="section-header">
          <h3 class="section-title" data-text="">Challenges</h3> -->
          <!-- TBD. -->
          <!--
          <table class="table">
            <caption style="font-weight: bolder;text-align: center;color: black;font-size:1.3em"><br><br>Challenge Submission<br><br></caption>

            <thead style="font-size: 1.2em">
              <tr>
                <td style="font-weight: bolder;color: black;vertical-align: middle;">Important Dates</td>
              </tr>
            </thead>
            <tbody style="font-size: 1.3em ">

            </tr>
              <tr> -->
              <!--&lt;!&ndash; <td><ul> <font style="font-weight: bold;">Track1~3</font> Due Date (mm/dd/yyyy): 06/04/2018 23:59 UTC/GMT+0</ul></td> &ndash;&gt;-->
              <!--
                <td>
                  <ul>Challenge Submission Due Date: <s>May 20, 2020, 00:00 AM UTC</s>
                  <font color="red">Extend to June 6, 2020, 00:00 AM UTC</font> -->
                  <!--<tr>Our new website is: </tr><br>-->
                  <!--<a href="http://47.100.21.47:9999/index.php"  target="_blank"> http://47.100.21.47:9999/index.php </a>-->
              <!--
                  <br></ul></td>
              </tr>
              -->
              <!--&lt;!&ndash; <tr>-->
              <!--<td><ul> <font style="font-weight: bold;">Track4~5</font> Due Date (mm/dd/yyyy): 05/31/2018 23:59 UTC/GMT+0<br><br></ul></td>-->
              <!--</tr> &ndash;&gt;-->
              <!---->
              <!--
            </tbody>
          </table>
          <div class="list-group">
            <a href="https://competitions.codalab.org/competitions/23431"  target="_blank" class="list-group-item">
              <h4 class="list-group-item-heading" style="font-weight: bolder;">
                Track1
              </h4>
              <p class="list-group-item-text">
                Look Into Person: Multi-Person Human Parsing Challenge
              </p>
            </a>
              <a href="https://competitions.codalab.org/competitions/23433"  target="_blank" class="list-group-item">
              <h4 class="list-group-item-heading" style="font-weight: bolder;">
                Track2
              </h4>
              <p class="list-group-item-text">
                Look Into Person: Video Mutil-Person Human Parsing Challenge
              </p>
            </a>
              <a href="https://competitions.codalab.org/competitions/23471"  target="_blank" class="list-group-item">
              <h4 class="list-group-item-heading" style="font-weight: bolder;">
                Track3
              </h4>
              <p class="list-group-item-text">
                Look Into Person: Image-based Multi-pose Virtual Try-on Challenge
              </p>
            </a>
            <a href="https://competitions.codalab.org/competitions/23472" target="_blank"  class="list-group-item">
              <h4 class="list-group-item-heading" style="font-weight: bolder;">
                Track4
              </h4>
              <p class="list-group-item-text">
                Look Into Person: Video Virtual Try-on Challenge
              </p>
            </a>
            <a href="https://competitions.codalab.org/competitions/24206" target="_blank"  class="list-group-item">
              <h4 class="list-group-item-heading" style="font-weight: bolder;">
                Track5
              </h4>
              <p class="list-group-item-text">
                Look Into Person: Dark Complexion Portrait Segmentation Challenge
              </p>
            </a>
            -->
            <!--<a href="http://sysu-hcp.net/lip"  target="_blank" class="list-group-item">-->
              <!--<h4 class="list-group-item-heading" style="font-weight: bolder;">-->
                <!--Track5-->
              <!--</h4>-->
              <!--<p class="list-group-item-text" >-->
                <!--Look Into Person: Image-based Multi-pose Virtual Try-on-->
              <!--</p>-->
            <!--</a>-->
            <!-- <a href="https://lv-mhp.github.io/" target="_blank" class="list-group-item">
              <h4 class="list-group-item-heading" style="font-weight: bolder;">
                Track4
              </h4>
              <p class="list-group-item-text">
                Pose Estimation
              </p>
            </a> -->
            <!--<a href="https://lv-mhp.github.io/" target="_blank" class="list-group-item">-->
              <!--<h4 class="list-group-item-heading" style="font-weight: bolder;">-->
                <!--Track4-->
              <!--</h4>-->
              <!--<p class="list-group-item-text">-->
                <!--Look Into Person: Multi-Human Pose Estimation Challenge(25403 images)-->
              <!--</p>-->
            <!--</a>-->
            <!--<a href="https://lv-mhp.github.io/" target="_blank" class="list-group-item">-->
              <!--<h4 class="list-group-item-heading" style="font-weight: bolder;">-->
                <!--Track5-->
              <!--</h4>-->
              <!--<p class="list-group-item-text">-->
                <!--Look Into Person: Fine-Grained Multi-Human Human Parsing Challenge(25403 images)-->
              <!--</p>-->
            <!--</a>-->
            <!--<a class="list-group-item">-->
              <!--<div class="row">-->
                  <!--<div class="col-md-6 col-xs-6  col-md-offset-1 " href="http://sysu-hcp.net/lip"  target="_blank">-->
                    <!--<img src="img/SYN.png" class="list-group-item-heading" style="padding-top: 2em" >-->
                  <!--</div>-->
                  <!--&lt;!&ndash;<div class="col-md-2 col-xs-6 " href="https://lv-mhp.github.io/" target="_blank">&ndash;&gt;-->
                    <!--&lt;!&ndash;<img src="img/FENGG.png" class="list-group-item-heading" style="padding-bottom: 2em " >&ndash;&gt;-->
                <!--&lt;!&ndash;</div>&ndash;&gt;-->
              <!--</div> -->
            <!--</a>   -->
            <!--
          </div>
        </header>
      </div>
    </div>
  </div>
</section>
-->

<!-- Schedule -->
<section id="schedule" class="schedule">
  <div class="container">
    <div class="row">
      <div class="col-md-12">
        <header class="section-header">
          <h3 class="section-title" data-text="" style="text-align: center;">Schedule</h3>
        </header>
        <body>
        <!--
          <table class="table table-hover table-condesed">
            <colgroup>
              <col width="40%">
              <col width="60%">
            </colgroup>

            <!-- <thead style="font-size: 1.3em">
              <tr  class="blue_bottom">
                <th>
                  <h4 style="font-weight: bolder"><a href="https://zoom.us/j/95207927547?pwd=VGdkSWdVVTBmQk9pZmw4djhNTDFSdz09" target="_blank" style="white-space: pre-wrap;">link.</a></h4>
                </th>
              </tr>
             </thead> -->
             <!--
             <thead style="font-size: 1.3em">
              <tr  class="blue_bottom">
                <th>
                  <h4 style="font-weight: bolder;text-align: center;color: #165ac5">Time</h4>
                </th>
                <th>
                  <h4 style="font-weight: bolder;color: #165ac5">Schedule</h4>
                </th>
              </tr>
             </thead>
            <tbody style="font-size: 1.2em">
              <tr>
                <td style="color:red; text-align: center;font-weight:bolder;">Location: </td>
                <td style="color:red;">Date: Friday, 19 June 2020 from 13:20 pm PDT to 18:30 pm PDT. (All times are Pacific Daylight Time, Seattle time).</td>
              </tr>
              <!-- <tr>
                <td style="text-align: center;font-weight:bolder;"><ul>Conference Country: United States of America</ul></td>
              </tr> -->
              <!--
              <tr>
               <td style="text-align: center;font-weight:bolder;">13:20-13:40</td>
               <td>Opening remarks and best paper talk [<a style="color: #165ac5;cursor:pointer;" href="https://youtu.be/iKFFEpj8qUo">YouTube Video1</a>] [<a style="color: #165ac5;cursor:pointer;" href="https://www.bilibili.com/video/BV1QD4y1D7TC/">Bilibili Video1</a>] [<a style="color: #165ac5;cursor:pointer;" href="https://youtu.be/vfNkKEcA3oU">YouTube Video2</a>]</td>
              </tr>
              <!-- <tr>
                <td style="text-align: center;font-weight:bolder;">13:40-14:20</td>
                <td>Invited talk 1: <b>Ira Kemelmacher-Shlizerman, Professor, University of Washington</b></td>
              </tr> -->
              <!--
              <tr>
                <td style="text-align: center;font-weight:bolder;">13:40-14:20</td>
                <td>Invited talk 1: <b>Ira Kemelmacher-Shlizerman, Associate Professor, University of Washington</b> [<a style="color: #165ac5;cursor:pointer;" href="https://youtu.be/f01nJqDbBVY">YouTube</a>] [<a style="color: #165ac5;cursor:pointer;" href="https://www.bilibili.com/video/BV1GK4y147qn/">Bilibili</a>]<br>Talk title: Human Modeling and Synthesis</td>
              </tr>
              <!-- <tr>
               <td style="text-align: center;font-weight:bolder;">14:20:15:00</td>
               <td>Invited talk 2: <b>Ming-Hsuan Yang, Professor, University of California at Merced</b></td>
              </tr> -->
              <!--
              <tr>
                <td style="text-align: center;font-weight:bolder;">14:20:15:00</td>
                <td>Invited talk 2: <b>William T. Freeman, Professor, MIT</b> [<a style="color: #165ac5;cursor:pointer;" href="https://youtu.be/B28coO-fo3o">YouTube</a>] [<a style="color: #165ac5;cursor:pointer;" href="https://www.bilibili.com/video/BV1gv411B7hs/">Bilibili</a>]<br>Talk title: Learning from videos playing forwards, backwards, fast, and slow
                  <div><a id="talk2" style="color: #165ac5;cursor:pointer;" onclick="tr_none('talk2');">Show Abstract</a></div>
                </td>
               </tr>
               <tr style="display:none;" class="talk2">
                <td style="text-align: center;vertical-align: middle;"></td>
                <td>Abstract: How can we tell that a video is playing backwards?  People's motions look wrong when the video is played backwards--can we develop an algorithm to distinguish forward from backward video?  Similarly, can we tell if a video is sped-up?<br>
                  We have developed algorithms to distinguish forwards from backwards video, and fast from slow.  Training algorithms for these tasks provides a self-supervised task that facilitates human activity recognition.  We'll show these results, and applications of these unsupervised video learning tasks.<br>
                  Joint work with: Donglai Wei, Joseph Lim, Andrew Zisserman, Sagie Benaim, Ariel Ephrat, Oran Lang, Inbar Mosseri, Michael Rubinstein, Michal Irani, Tali Dekel
                </td>
              </tr>
               <tr>
                <td style="text-align: center;font-weight:bolder;">15:00-15:15</td>
                <!-- <td>Winner talk 1: Winner of the Multi-Person Human Parsing Challenge<br>Talk title: QANet for Multiple Human Parsing</td> -->
                <!--
                <td>Winner talk 1: Winner of the Multi-Person Human Parsing Challenge [<a style="color: #165ac5;cursor:pointer;" href="https://youtu.be/-eVzj86-kLk">YouTube</a>] [<a style="color: #165ac5;cursor:pointer;" href="https://www.bilibili.com/video/BV1jA411i7Mo/">Bilibili</a>] [<a style="color: #165ac5;cursor:pointer;" href="https://drive.google.com/file/d/1GPPL6aediBJJXglezSRk_DAhzuNsqkqq/view?usp=sharing">Slide</a>]</td>
              </tr>
              <tr>
                <td style="text-align: center;font-weight:bolder;">15:15-15:30</td>
                <!-- <td>Winner talk 2: Winner of the Video Multi-Person Human Parsing Challenge<br>Talk title: QANet for Multiple Human Parsing</td> -->
                <!--
                <td>Winner talk 2: Winner of the Video Multi-Person Human Parsing Challenge [<a style="color: #165ac5;cursor:pointer;" href="https://youtu.be/-eVzj86-kLk">YouTube</a>] [<a style="color: #165ac5;cursor:pointer;" href="https://www.bilibili.com/video/BV1jA411i7Mo/">Bilibili</a>] [<a style="color: #165ac5;cursor:pointer;" href="https://drive.google.com/file/d/1GPPL6aediBJJXglezSRk_DAhzuNsqkqq/view?usp=sharing">Slide</a>]</td>
              </tr>
              <!--<tr>-->
                <!--<td style="text-align: center;font-weight:bolder;">09:15-10:00</td>-->
                <!--<td>Invited talk 1: <strong>Alan L. Yuille, Professor, Johns Hopkins University</strong></td>-->
              <!--</tr>-->
              <!-- <tr>
               <td style="text-align: center;font-weight:bolder;">15:30-16:10</td>
               <td>Invited talk 3: <b>Jun-Yan Zhu, Professor, Carnegie Mellon University</b></td>
              </tr> -->
              <!--
              <tr>
                <td style="text-align: center;font-weight:bolder;">15:30-16:10</td>
                <td>Invited talk 3: <b>Ming-Hsuan Yang, Professor, University of California at Merced</b> [<a style="color: #165ac5;cursor:pointer;" href="https://youtu.be/XLfvrlH8OaM">YouTube</a>] [<a style="color: #165ac5;cursor:pointer;" href="https://www.bilibili.com/video/BV1Tf4y1177w/">Bilibili</a>]<br>Talk title: Synthesizing Human Images in 2D and 3D Scenes
                <div><a id="talk3" style="color: #165ac5;cursor:pointer;" onclick="tr_none('talk3');">Show Abstract</a></div>
                </td>
              </tr>
              <tr style="display:none;" class="talk3">
                <td style="text-align: center;vertical-align: middle;"></td>
                <td>Abstract: In this talk, I will present our recent results on synthesizing human images in 2D and 3D scenes. In the first part, I will present a context-aware approach to synthesize and place object instances in an image with semantically coherent contents. In the second part, I will describe a method to synthesizing 3D humans with varying pose in indoors in an image by inferring 3D layout and context. When time allows, I will also present an algorithm to model music-to-dance generation process for synthesizing realistic, diverse, style-consistent, and beat-matching dances from music.</td>
              </tr>

              <tr>
                <td style="text-align: center;font-weight:bolder;vertical-align: middle;">16:10-16:50</td>
                <td >Invited talk 4: <b>Jun-Yan Zhu, Assistant Professor, Carnegie Mellon University</b> [<a style="color: #165ac5;cursor:pointer;" href="https://youtu.be/CV8VogSpmFY">YouTube</a>] [<a style="color: #165ac5;cursor:pointer;" href="https://www.bilibili.com/video/BV1pK4y147eU/">Bilibili</a>]<br>Talk title: Visualizing and Understanding GANs
                  <div><a id="talk4" style="color: #165ac5;cursor:pointer;" onClick="tr_none('talk4');">Show Abstract</a></div>
                </td>
              </tr>
              <tr style="display:none;" class="talk4">
                <td style="text-align: center;vertical-align: middle;"></td>
                <td>Abstract: Generative Adversarial Networks (GANs) have recently achieved impressive results for a wide range of real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, they have not been well visualized or understood. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models. In this talk, I will present several analytic tools to visualize and understand GANs at the unit-, object-, and scene-level. Collectively, these tools highlight what a GAN has learned and has not. We show several practical applications enabled by our method, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in a real image.</td>
              </tr>
              <!--<tr>-->
                <!--<td style="text-align: center;font-weight:bolder;vertical-align: middle;">10:30-11:15</td>-->
                <!--<td>Invited talk 2: <strong>Alexei (Alyosha) Efros, Professor, UC Berkeley</strong></td>-->
              <!--</tr>-->
              <!-- <tr>
                <td style="text-align: center;font-weight:bolder;">11:15-11:45</td>
                <td>Invited talk 3: Alan Yuille, Johns Hopkins University</td>
              </tr> -->
              <!--
              <tr>
               <td style="text-align: center;font-weight:bolder;">16:50-17:05</td>
                <!-- <td>Winner talk 3: Winner of the Image-based Multi-pose Virtual Try-on Challenge<br>Talk title: FashionOn: Semantic-guided Image-based Virtual Try-on with Detailed Human and Clothing Information</td> -->
                <!--
                <td>Winner talk 3: Winner of the Image-based Multi-pose Virtual Try-on Challenge [<a style="color: #165ac5;cursor:pointer;" href="https://youtu.be/zloK9g6RvYk">YouTube</a>] [<a style="color: #165ac5;cursor:pointer;" href="https://www.bilibili.com/video/BV1f54y1B7VJ/">Bilibili</a>] [<a style="color: #165ac5;cursor:pointer;" href="https://drive.google.com/file/d/1fE3p1zSAOaiU-trMM6-dvL2Klcm1Goih/view?usp=sharing">Slide</a>]</td>
              </tr>
              <tr>
                <td style="text-align: center;font-weight:bolder;">17:05-17:20</td>
                 <!-- <td>Winner talk 4: Winner of the Video Virtual Try-on Challenge<br>Talk title: Video Virtual Try-on Challenge Talk</td> -->
                 <!--
                 <td>Winner talk 4: Winner of the Video Virtual Try-on Challenge [<a style="color: #165ac5;cursor:pointer;" href="https://youtu.be/X7DPS-G1n0I">YouTube</a>] [<a style="color: #165ac5;cursor:pointer;" href="https://www.bilibili.com/video/BV1tg4y1q7Sc/">Bilibili</a>] [<a style="color: #165ac5;cursor: pointer;" href="https://drive.google.com/file/d/1mhss9tC2L2e_HqZLPdxrbcxY0MpFp01A/view?usp=sharing">Slide</a>]</td>
                </tr>
               <tr>
                <td style="text-align: center;font-weight:bolder;">17:20-17:35</td>
                 <!-- <td>Winner talk 5: Winner of the Dark Complexion Portrait Segmentation Challenge<br>Talk title: Dark Complexion Portrait Segmentation Challenge Talk</td> -->

                 <!--
                 <td>Winner talk 5: Winner of the Dark Complexion Portrait Segmentation Challenge [<a style="color: #165ac5;cursor:pointer;" href="https://youtu.be/TOxvrkh6kUY">YouTube</a>] [<a style="color: #165ac5;cursor:pointer;" href="https://www.bilibili.com/video/BV1dD4y1D7wp/">Bilibili</a>] [<a style="color: #165ac5;cursor:pointer;" href="https://drive.google.com/file/d/1aC3WbMca6PaZZHthGjEXfqoovJrLhAmL/view?usp=sharing">Slide</a>]</td>
                </tr>
              <tr>
                <td style="text-align: center;font-weight:bolder;">17:35-18:30</td>
                <td>Oral: Epipolar Transformer for Multi-view Human Pose Estimation.</td>
              </tr>
              <tr>
                <td style="text-align: center;font-weight:bolder;">17:35-18:30</td>
                <td>Oral: Yoga-82: A New Dataset for Fine-grained Classification of Human Poses.</td>
              </tr>
              <tr>
                <td style="text-align: center;font-weight:bolder;">17:35-18:30</td>
                <td>Oral: The MTA Dataset for Multi Target Multi Camera Pedestrian Tracking by Weighted Distance Aggregation.</td>
              </tr>
              <tr>
                <td style="text-align: center;font-weight:bolder;">17:35-18:30</td>
                <td>Poster: LightTrack: A Generic Framework for Online Top-Down Human Pose Tracking.</td>
              </tr>
              <tr>
                <td style="text-align: center;font-weight:bolder;">17:35-18:30</td>
                <td>Poster: Fine grained pointing recognition for natural drone guidance.</td>
              </tr>
              <tr>
                <td style="text-align: center;font-weight:bolder;">17:35-18:30</td>
                <td>Poster: Reposing Humans by Warping 3D Features.</td>
              </tr>

              <!-- <tr>
               <td style="text-align: center;font-weight:bolder;">11:15-11:30</td>
               <td>Oral talk 2: Winner of single-person pose estimation challenge</td>
              </tr>
              <tr>
                <td style="text-align: center;font-weight:bolder;">11.30-11.45</td>
                <td>Oral talk 3: Winner of multi-person human parsing challenge</td>
              </tr> -->
              <!-- <tr>
                <td style="text-align: center;font-weight:bolder;">11:45-12:00</td>
                <td>Oral talk 4: Winner of video parsing challenge</td>
              </tr> -->
              <!-- <tr>
                <td style="text-align: center;font-weight:bolder;">14:00-14:30</td>
                <td>Invited talk 3: Trevor Darrell, UC Berkeley</td>
              </tr> -->
              <!-- <tr>
                <td style="text-align: center;font-weight:bolder;">17:30-18:00</td>
                <td>Awards & Future Plans</td>
              </tr> -->
              <!-- <tr>
               <td style="text-align: center;font-weight:bolder;">14:00-14:15</td>
               <td>Oral talk 2: Winner of single-person(Track 3) & multi-person(Track 4) pose estimation challenge, Speaker: Wu Liu(JD AI Research)</td>
              </tr>
              <tr>
                <td style="text-align: center;font-weight:bolder;">14.15-14.30</td>
                <td>Oral talk 3: Winner of single-person(Track 1) & multi-person(Track 2 & Track 5) human parsing challenge, Speaker: Yunchao Wei(University of Illinois Urbana-Champaign)</td>
              </tr> -->
              <!-- <tr>
                <td style="text-align: center;font-weight:bolder;">14:00-14:30</td>
                <td>Invited talk 4: <strong>Jianchao Yang, Director, ByteDance AI Lab.</strong></td>
              </tr>
              <tr>
                <td style="text-align: center;font-weight:bolder;">14:30-14:45</td>
                <td>Oral talk 4: Winner of image-based multi-pose virtual try-on challenge</td>
              </tr>
              <tr>
                <td style="text-align: center;font-weight:bolder;">14:45-16:15</td>
                <td>Poster session and coffee break</td>
              </tr> -->
              <!--<tr>-->
               <!--<td style="text-align: center;font-weight:bolder;">15:15-15:45</td>-->
                <!--<td>Invited talk 6: <strong>Katerina Fragkiadaki, Assistant Professor, CMU</strong></td>-->
              <!--</tr>-->
              <!-- <tr>
               <td style="text-align: center;font-weight:bolder;">16:15-16:45</td>
                <td>Invited talk 5: <strong>Katerina Fragkiadaki, Assistant Professor, CMU</strong></td>
              </tr>
              <tr>
                <td style="text-align: center;font-weight:bolder;">16:45-17:15</td>
                <td>Awards & Future Plans</td>
              </tr> -->
              <!--<tr>-->
                <!--<td style="text-align: center;font-weight:bolder;">15:45-16:15</td>-->
                <!--<td>Invited talk 7: <strong>Chunhua Shen, Professor, University of Adelaide</strong></td>-->
              <!--</tr>-->
              <!--<tr>-->
                <!--<td style="text-align: center;font-weight:bolder;">16:15-16:45</td>-->
                <!--<td>Awards & Future Plans</td>-->
              <!--</tr>-->
            <!-- </tbody>
          </table> -->
        <!-- TBD. -->

        <table class="table table-hover table-condesed">
          <tbody>
            <tr>
              <td style="font-weight: bolder;color: black;font-size:1.1em;vertical-align: middle; width:26%;">Time: July 14, 2023 (Friday) UTC+10</td>
              <td style="font-weight: bolder;color: black;font-size:1.1em;vertical-align: middle;"></td>
            </tr>
            <tr>
              <td>9:00 a.m.-10:00 a.m.</td>
              <td><b>Invited Keynote: Environmental Intelligence Unleashed: Harnessing the Power of Robot Teams</b><br><i>Peyman Moghadam (CSIRO, Australia)</i><br></td>
            </tr>
            <tr>
              <td>10.00 a.m.-10:30 a.m.</td>
              <td><b>Short Break</b><br></td>
            </tr>
            <tr>
              <td>10:30 a.m.-11:50 a.m.<br><i>(max. 20 mins per talk)</i></td>
              <td>Simultaneous Super Resolution and Moving Object Detection from Low Resolution Surveillance Videos<br>
              <i>Anju Jose Tom, Sudhish George<br>(National Institute of Technology Calicut, India)</i>
              <br><br>
              A System for Real-time Recognition and Fast Retrieval of Close Contacts in Multi-Camera Videos<br>
              <i>Wenjie Yang, Yang Zhang, Zhenyu Xie<br>(Shanghai Jiao Tong University, China)</i>
              <br><br>
              Cross-level Guided Attention for Human-Object Interaction Detection<br>
              <i>Zongxu Yue, Ge Li, Wei Gao<br>(Shenzhen Graduate School, Peking University, China)</i>
              <br><br>
              Automatic Defect Detection in Wind Turbine Blade Images: Model Benchmark and Re-Annotations<br>
              <i>Imad Gohar, John See, Abderrahim Halimi, Weng Kean Yew<br>(Heriot-Watt University Malaysia)</i>
              <br><br>
              </td>
            </tr>
          </tbody>
        </table>

        </body>
      </div>
    </div>
  </div>
</section>



<!-- ********************************************************************************************************** -->

<HR style="FILTER: alpha(opacity=100,finishopacity=0,style=3)" width="90%" color=#987cb9 SIZE=3>

<section id="speaker" class="speaker">
  <div class="container">
    <div class="row">
      <div class="col-md-12">
        <header class="section-header">
          <h3 class="section-title" data-text="">Invited Keynote Speaker</h3>
          <div class="col-sm-3 col-md-offset-1">
              <img src="img/speakers/peyman.jpg" class="img-circle img-responsive img-thumbnail center-block" alt="img">
              <h5 class="member-name"  data-text=""><a href="https://people.csiro.au/m/p/peyman-moghadam" target="_blank" style="white-space: pre-wrap;">PEYMAN MOGHADAM</a></h5>
          </div>
          <div class="col-sm-6">
              <h3>Environmental Intelligence Unleashed: Harnessing the Power of Robot Teams</h3>
              <!-- <p>
                <b>Abstract</b>: Video-analytics-as-a-service enables a wide range of real-world applications, e.g., video surveillance, smart shopping systems like Amazon Go, elderly person monitoring systems. A key concern in such services is the privacy of the videos being analyzed, as analyzing such information-rich video data may reveal personal information like an individual’s daily routine, home location, gender, race, clothes, etc. Therefore, there is a pressing need for solutions to privacy-preserving video analysis. In this talk, we will present our recent work on a novel self-supervised privacy-preserving action recognition framework. It removes privacy information from input video in a self-supervised manner without requiring privacy labels. Extensive experiments show that our framework achieves competitive performance compared to the supervised baseline for the known action privacy attributes. We also showed that our method achieves better generalization to novel action-privacy attributes compared to the supervised baseline.
              </p> -->
              <p>
                <b>Biodata</b>: Dr. Peyman Moghadam is a Principal Research Scientist at CSIRO Data61, Adjunct Professor at the Queensland University of Technology (QUT), and Adjunct Associate Professor at the University of Queensland (UQ). He is leading the Embodied AI research cluster at the CSIRO Robotics and Autonomous Systems group working at Intersection of Robotics and Machine learning. As the leader of Spatiotemporal portfolio at CSIRO's Machine Learning and Artificial Intelligence (MLAI) Future Science Platform, Dr Moghadam also oversees research and development of MLAI methods for scientific discovery in spatiotemporal data streams. Before joining CSIRO, Peyman worked in a number of world leading organisations such as the Deutsche Telekom Laboratories (Germany) and the Singapore-MIT Alliance for Research and Technology (Singapore). Dr Moghadam has led several large-scale multidisciplinary projects and won numerous awards for his innovations, including CSIRO Julius Career award, National, and Queensland iAward for Research and Development, and the Lord Mayor’s Budding Entrepreneurs Award. In 2019, he held a Visiting Scientist appointment at the Agricultural Robotics and Engineering group at the University of Bonn, as part of the CSIRO Julius Career Award. His current research interests include self-supervised learning for robotics, embodied AI, 3D multi-modal perception (3D++), robotics, computer vision, deep learning, and 3D thermal/hyperspectral imaging.
          </div>
        </header>
      </div>
    </div>

  </div>
</section>


<!-- ********************************************************************************************************** -->
<HR style="FILTER: alpha(opacity=100,finishopacity=0,style=3)" width="90%" color=#987cb9 SIZE=3>
<!-- Team -->
<section id="team" class="team">
  <div class="container">
    <div class="row">
      <div class="col-md-12">
        <header class="section-header">
          <h3 class="section-title" data-text="">Organizers</h3>
        </header>
      </div>
    </div>

    <div class="row">
      <div class="col-sm-4 col-md-2 col-md-offset-2">
        <img src="img/team/johnsee.jpg" class="img-circle img-responsive img-thumbnail center-block" alt="img">
        <h5 class="member-name"  data-text="Heriot-Watt University (Malaysia)"><a href="http://john-see.github.io" target="_blank" style="white-space: pre-wrap;">John See</a><br><span class="fa-icon-envelope" style="white-space: pre-wrap;"><font size="-1" style="font-weight: normal;text-transform: lowercase;">&nbsp;J.See@hw.ac.uk</font></span></h5>
      </div>
      </div>
      <div class="col-sm-4 col-md-2 col-md-offset-1">
        <img src="img/team/minxian.jpg" class="img-circle img-responsive img-thumbnail center-block" alt="img">
        <h5 class="member-name"  data-text="Nanjing University of Science and Technology, China"><a href="https://liminxian.github.io/" target="_blank" style="white-space: pre-wrap;">Minxian Li</a><br><span class="fa-icon-envelope" style="white-space: pre-wrap;"><font size="-1" style="font-weight: normal;text-transform: lowercase;">&nbsp;minxianli@njust.edu.cn</font></span></h5>
      </div>
      <div class="col-sm-4 col-md-2 col-md-offset-1">
          <img src="img/team/saimun.jpg" class="img-circle img-responsive img-thumbnail center-block" alt="img">
          <h5 class="member-name"   data-text="CSIRO, Australia"><a href="https://documents.uow.edu.au/~sr801/" target="_blank" style="white-space: pre-wrap;">Saimunur Rahman</a><br><span class="fa-icon-envelope" style="white-space: pre-wrap;"><font size="-1" style="font-weight: normal;text-transform: lowercase;">&nbsp;saimun.rahman@data61.csiro.com</font></span></h5>
        </div>
    </div>
    <!--
    <div class="row">
      <div class="col-sm-4 col-md-2 col-md-offset-2">
            <img src="img/team/linliang.jpg" class="img-circle img-responsive img-thumbnail center-block" alt="img">
          <h5 class="member-name"  data-text="Professor, Sun Yat-sen University"><a href="http://www.linliang.net/" target="_blank" style="white-space: pre-wrap;">Liang Lin</a><br><span class="fa-icon-envelope" style="white-space: pre-wrap;"><font size="-1" style="font-weight: normal;text-transform: lowercase;">&nbsp;linliang@ieee.org</font></span></h5>
      </div>
      <div class="col-sm-4 col-md-2 col-md-offset-1">
          <img src="img/team/Jiashi Feng.png" class="img-circle img-responsive img-thumbnail center-block" alt="img">
        <h5 class="member-name"  data-text="Assistant Professor, National University of Singapore"><a href="https://sites.google.com/site/jshfeng/" style="white-space: pre-wrap;">Jiashi Feng</a><br><span class="fa-icon-envelope" style="white-space: pre-wrap;"><font size="-1" style="font-weight: normal;text-transform: lowercase;">&nbsp;elefjia@nus.edu.sg</font></span></h5>
      </div>
      <div class="col-sm-4 col-md-2 col-md-offset-1">
            <img src="img/team/songchun.jpg" class="img-circle img-responsive img-thumbnail center-block" alt="img">
          <h5 class="member-name"   data-text="Professor, University of California, Los Angeles"><a href="http://www.stat.ucla.edu/~sczhu" target="_blank" style="white-space: pre-wrap;">Song-Chun Zhu</a><br><span class="fa-icon-envelope" style="white-space: pre-wrap;"><font size="-1" style="font-weight: normal;text-transform: lowercase;">&nbsp;sczhu@stat.ucla.edu</font></span></h5>
        </div>
    </div>
    -->
  </div>
</section>


<HR style="FILTER: alpha(opacity=100,finishopacity=0,style=3)" width="90%" color=#987cb9 SIZE=3>
<!-- Blog -->
<section id="association" class="association">
  <div class="container">
    <div class="row">
      <div class="col-md-12">
        <header class="section-header">
          <h3 class="section-title" data-text="">Contact</h3>
          <p style="text-align: center;font-size: 1.2em;white-space: pre-wrap">Please feel free to send any question or comments to: <br><font color=#0E04F5>j DOT see AT hw.ac.uk</font>
          </p>
        <!--  <p style="text-align: left;font-size: 1.2em;white-space: pre-wrap; margin-top: -50px">
            &nbsp;&nbsp;&nbsp;&nbsp;You are also welcomed to discuss with us in our googlegroup:<font style="color: blue">lip17-organizers@googlegroups.com</font>
          </p>-->

        </header>
      </div>
    </div>
  </div>
</section>

<!-- Footer -->
<footer class="footer">
  <div class="container">
    <div class="row">
      <div class="col-md-12 col-sm-12 center-block" data-animation="zoomIn" data-animation-delay="01">
        <!-- <h6><a href="http://sysu-hcp.net/lip">L.I.P</a></h6> -->

<!--        <ul class="social-list">
          <li class="fa-icon-facebook"></li>
          <li class="fa-icon-twitter"></li>
          <li class="fa-icon-pinterest"></li>
          <li class="fa-icon-flickr"></li>
          <li class="fa-icon-dribbble"></li>
          <li class="fa-icon-behance"></li>
        </ul> -->
      </div>
    </div>
  </div>
</footer>


<script src="js/jquery.js"></script>
<script src="js/vendor/fastclick.js"></script>

<script src="js/bootstrap.min.js"></script>

<script src="js/vendor/jquery.appear.js"></script>                  <!-- jQuery Appear -->
<script src="js/vendor/jquery.easing.1.3.js"></script>              <!-- jQuery Easing -->
<script src="js/vendor/imagesloaded.pkgd.min.js"></script>          <!-- Imagesloaded -->
<script src="js/vendor/isotope.pkgd.min.js"></script>               <!-- Isotope -->
<script src="js/vendor/jquery.countTo.js"></script>                 <!-- Count To -->
<script src="js/vendor/jquery.easypiechart.min.js"></script>        <!-- easyPieChart -->
<script src="js/vendor/jquery.magnific-popup.min.js"></script>      <!-- Magnific Popup -->
<script src="js/vendor/owl.carousel.min.js"></script>               <!-- Owl Carousel -->
<script src="js/vendor/jquery.validate.min.js"></script>            <!-- jQuery Validate -->
<script src="js/contact.js"></script>

<script type="text/javascript" src="js/sliders/jquery.themepunch.tools.min.js"></script>
<script type="text/javascript" src="js/sliders/jquery.themepunch.revolution.min.js"></script>

<!-- SLIDER REVOLUTION 5.0 EXTENSIONS  (Load Extensions only on Local File Systems !  The following part can be removed on Server for On Demand Loading) -->
<script type="text/javascript" src="js/sliders/revolution.extension.actions.min.js"></script>
<script type="text/javascript" src="js/sliders/revolution.extension.carousel.min.js"></script>
<script type="text/javascript" src="js/sliders/revolution.extension.kenburn.min.js"></script>
<script type="text/javascript" src="js/sliders/revolution.extension.layeranimation.min.js"></script>
<script type="text/javascript" src="js/sliders/revolution.extension.migration.min.js"></script>
<script type="text/javascript" src="js/sliders/revolution.extension.navigation.min.js"></script>
<script type="text/javascript" src="js/sliders/revolution.extension.parallax.min.js"></script>
<script type="text/javascript" src="js/sliders/revolution.extension.slideanims.min.js"></script>
<script type="text/javascript" src="js/sliders/revolution.extension.video.min.js"></script>

<script src="js/main.js"></script>                                  <!-- Custom jQuery -->
<script src="js/functions.js"></script>                             <!-- Revolution Functions -->

<script language="javascript">
  function tr_none(theclass){
    var allPageTags = new Array();
    var allPageTags = document.getElementsByTagName("tr");
    var hrefTag = document.getElementById(theclass)
    for (i = 0; i<allPageTags.length; i++) {
      if (allPageTags[i].className == theclass) {
        var obj = allPageTags[i];
        if (obj.style.display == "none") {
          obj.style.display = "";
          hrefTag.innerHTML = "Hide Abstract"
        } else {
          obj.style.display = "none";
          hrefTag.innerHTML = "Show Abstract"
        }
      }
    }
  }
</script>

</body>
</html>
